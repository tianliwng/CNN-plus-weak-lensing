{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "[449 449] [512 512]\n"
     ]
    }
   ],
   "source": [
    "from astropy.io import fits\n",
    "from astropy.table import Table\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "\n",
    "import time\n",
    "import os\n",
    "\n",
    "######################## define constants etc. ############################### \n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "#rootpath = '/n/holyscratch01/dvorkin_lab/Users/tianliwang/maps_unzipped/'  # location of maps \n",
    "rootpath = '/Volumes/WTL/convergence_maps/All_cosm/'  # <-- Tianli's testing file path\n",
    "workPath = '/Users/Ed/Desktop/cosm_prjt/'  # path to load the net from and write the predictions in\n",
    "\n",
    "# these parameters are defined in the paper  \n",
    "n_imagespercosmo = 512  # number of images per cosmology \n",
    "n_perbatch = 32         # batch size for training \n",
    "n_perbatch_test = n_perbatch  # the value shoudn't matter but put here in case we need to change things\n",
    "n_batch = n_imagespercosmo/n_perbatch  # number of minibatches \n",
    "dim_image = 1024 \n",
    "dim_downsized = int(dim_image/2)            # downsized image dimension \n",
    "\n",
    "trainFraction = 14.0/16  # fraction of images per cosmology for training \n",
    "validateFraction = 0.0/16\n",
    "testFraction = 1-trainFraction-validateFraction  # needs more cleaning up \n",
    "\n",
    "n_cosmosToTrain = 2  # same for validation and testing\n",
    "n_trainimages = int(n_imagespercosmo*trainFraction)    # number of images per cosmology to train\n",
    "n_validateimages = int(n_imagespercosmo*validateFraction)   # number of images per cosmology to validate\n",
    "n_testimages = int(n_imagespercosmo*testFraction)  # number of images per cosmology to test\n",
    "n_batch_train = int(n_trainimages/n_perbatch)  # number of minibatches per cosmology to train \n",
    "\n",
    "# start and end image indices for training and validating \n",
    "startIndices_train = np.ones(n_cosmosToTrain).astype(int)\n",
    "endIndices_train = n_trainimages*np.ones(n_cosmosToTrain).astype(int)\n",
    "startIndices_validate = (n_trainimages+1)*np.ones(n_cosmosToTrain).astype(int)\n",
    "endIndices_validate = (n_trainimages+n_validateimages)*np.ones(n_cosmosToTrain).astype(int)\n",
    "startIndices_test = (n_trainimages+n_validateimages+1)*np.ones(n_cosmosToTrain).astype(int)\n",
    "endIndices_test = (n_trainimages+n_validateimages+n_testimages)*np.ones(n_cosmosToTrain).astype(int)\n",
    "\n",
    "print(startIndices_test, endIndices_test)  # shoud be 481 and 512 (just testing)\n",
    "\n",
    "\n",
    "######################## define functions & Class ###############################\n",
    "\n",
    "def read_parametersfromfile(path):\n",
    "    '''\n",
    "    Input: string for the directory to the folders of cosmology \n",
    "    \n",
    "    Function: reads the Omega_m and sigma_8 values from the folder names in \n",
    "    the path directory and store them as arrays of strings. Stores the values \n",
    "    as strings to avoid rounding, for example, 0.600 to 0.6. \n",
    "    \n",
    "    Output: (array of strings of Omega_m, array of strings of sigma_8)\n",
    "    '''\n",
    "    \n",
    "    Om_strings = [] \n",
    "    si_strings = []\n",
    "    \n",
    "    for filename in os.listdir(path):\n",
    "        if (filename.startswith('Om') and not filename.endswith('.gz')): \n",
    "            Om_strings.append(filename[2:7]) \n",
    "            si_strings.append(filename[10:15])\n",
    "            \n",
    "    return Om_strings, si_strings\n",
    "\n",
    "\n",
    "def read_files(path, Oms, sis, start_indices, end_indices): \n",
    "    '''\n",
    "    Input: file root directory path, strings of omegam and sigma8 values, \n",
    "    start image indices and end image indices. Oms, sis, start_indices, end_indices\n",
    "    should be arrays of same length. \n",
    "    \n",
    "    Function: reads the images labeled between start_indices[i] and end_indices[i] (inclusive)\n",
    "    for each of the ith cosmologies specified by omegam and sigma8 values \n",
    "    \n",
    "    Output: numpy array of 3D images\n",
    "    '''\n",
    "    \n",
    "    n_cosmos = len(Oms)  # this should be shared by Oms, sis, start_indices, end_indices \n",
    "    \n",
    "    # read in the files and put them into 3D matrix \n",
    "    filedata = []\n",
    "\n",
    "    for j in range(0, n_cosmos): \n",
    "        i_start = start_indices[j]\n",
    "        i_end = end_indices[j]\n",
    "        Om = Oms[j]\n",
    "        si = sis[j]\n",
    "        \n",
    "        for i in range(i_start, i_end+1): \n",
    "            if (i < 10): filenum = '00{}'.format(i)\n",
    "            elif (i < 100): filenum = '0{}'.format(i)\n",
    "            else: filenum = i \n",
    "\n",
    "            hdulist = fits.open('{}Om{}_si{}/WLconv_z1.00_0{}r.fits'.format(path, Om, si, filenum))\n",
    "            image = hdulist[0].data\n",
    "\n",
    "            # downsize from dim_image to dim_downsized \n",
    "            image_downsized = image.reshape([dim_downsized, dim_image//dim_downsized, \n",
    "                                    dim_downsized, dim_image//dim_downsized]).mean(3).mean(1)\n",
    "\n",
    "            # newaxis makes the image 3D (1x512x512) \n",
    "            filedata.append(image_downsized[np.newaxis,:,:])\n",
    "    \n",
    "    return np.array(filedata)\n",
    "           \n",
    "    \n",
    "def test_cnn(network, testinputs, batchsize, isTrackingloss, **kwargs): \n",
    "    '''\n",
    "    Input: CNN object, testing data, batch size for testing, boolean for whether \n",
    "    to track losses bewteen test ouputs and targets, **kwargs includes the loss function\n",
    "    criterion and the targetting outputs. Target should be in the form of \n",
    "    an array of (omegam, sigma8) parameters each corresponding to the target of one batch. \n",
    "    \n",
    "    Call with \n",
    "    test_cnn(network, testinputs, batchsize, False) \n",
    "    or \n",
    "    test_cnn(network, testinputs, batchsize, True, loss_fn=loss_fn, target=target)\n",
    "    \n",
    "    Function: Run the network with test inputs and specified batch size and track loss as \n",
    "    requested. \n",
    "    \n",
    "    Output: \n",
    "    If isTrackingloss is True, return (np array of predicted outputs, np array of losses for each batch)\n",
    "    If isTrackingloss is False, return np array of predicted outputs\n",
    "    '''\n",
    "    \n",
    "    # unpack optional arguments **kwargs \n",
    "    if (isTrackingloss): \n",
    "        loss_fn = kwargs.get('loss_fn', None)\n",
    "        testtargets = kwargs.get('target', None)\n",
    "        losses = []\n",
    "    \n",
    "    # make the data into 4D tensor and put each batch into iterable \n",
    "    testloader = torch.utils.data.DataLoader(testinputs, batch_size=batchsize, shuffle=False)\n",
    "    \n",
    "    predictions = []  # stores outputs of network with test data inputs \n",
    "    \n",
    "    with torch.no_grad(): \n",
    "        for i, testdata in enumerate(testloader, 0):\n",
    "            testdata = testdata.to(device)  # GPU\n",
    "            testoutputs = network(testdata)\n",
    "            if i==0:\n",
    "                predictions = (testoutputs.cpu()).numpy()\n",
    "            if i!=0:\n",
    "                predictions = np.concatenate((predictions, (testoutputs.cpu()).numpy()),0)\n",
    "#            predictions.append((testoutputs.cpu()).numpy())  # put tensor back into cpu before converting to np \n",
    "##            print(predictions)\n",
    "\n",
    "            if (isTrackingloss): \n",
    "                testtarget = torch.tensor(testtargets[i]).repeat(batchsize, 1)  \n",
    "##                print(testtarget)\n",
    "                testtarget = testtarget.to(device)  # GPU\n",
    "                loss = np.array([(loss_fn(testoutputs[i], testtarget[i])).cpu() for i in range(batchsize)])\n",
    "                losses.append(loss)\n",
    "##                print(loss)\n",
    "    \n",
    "    if (isTrackingloss): \n",
    "        return np.array(predictions), np.array(losses)\n",
    "    \n",
    "    return np.array(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network structure \n",
    "\n",
    "def conv_block(in_f, out_f, *args, **kwargs):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_f, out_f, *args, **kwargs),\n",
    "        nn.BatchNorm2d(out_f),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "\n",
    "class Net (nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.cnn_layers = nn.Sequential(\n",
    "            # layers 1+2 \n",
    "            conv_block(1, 32, kernel_size=3), \n",
    "            conv_block(32, 32, kernel_size=3), \n",
    "            nn.AvgPool2d(2, 2), \n",
    "            # layers 3+4\n",
    "            conv_block(32, 64, kernel_size=3), \n",
    "            conv_block(64, 64, kernel_size=3), \n",
    "            nn.AvgPool2d(2, 2), \n",
    "            # layers 5-7 \n",
    "            conv_block(64, 128, kernel_size=3),\n",
    "            conv_block(128, 64, kernel_size=1),\n",
    "            conv_block(64, 128, kernel_size=3),\n",
    "            nn.AvgPool2d(2, 2),\n",
    "            # layers 8-10 \n",
    "            conv_block(128, 256, kernel_size=3),\n",
    "            conv_block(256, 128, kernel_size=1),\n",
    "            conv_block(128, 256, kernel_size=3),\n",
    "            nn.AvgPool2d(2, 2),\n",
    "            # layers 11-13 \n",
    "            conv_block(256, 512, kernel_size=3),\n",
    "            conv_block(512, 256, kernel_size=1),\n",
    "            conv_block(256, 512, kernel_size=3),\n",
    "            nn.AvgPool2d(2, 2),\n",
    "            # layers 14-18 \n",
    "            conv_block(512, 512, kernel_size=3),\n",
    "            conv_block(512, 256, kernel_size=1),\n",
    "            conv_block(256, 512, kernel_size=3),\n",
    "            conv_block(512, 256, kernel_size=1),\n",
    "            nn.Conv2d(256, 512, kernel_size=3),\n",
    "            nn.ReLU(),             # no batch norm on last convolution layer \n",
    "            nn.AvgPool2d(6, 6)\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(512, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn_layers(x)\n",
    "        x = x.view(-1, 512)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Time to load the test files: 21.55128002166748 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# read in all the omegam and sigma8 values from the directory \n",
    "Omegam_strings, sigma8_strings = read_parametersfromfile(rootpath)\n",
    "\n",
    "Omegastrings_test = Omegam_strings[:n_cosmosToTrain]\n",
    "sigmastrings_test = sigma8_strings[:n_cosmosToTrain]\n",
    "Omegas_test = np.array(Omegastrings_test).astype(np.float)\n",
    "sigmas_test = np.array(sigmastrings_test).astype(np.float)\n",
    "\n",
    "start_time = time.time()  \n",
    "inputs_test = read_files(rootpath, Omegastrings_test, sigmastrings_test, \n",
    "                             startIndices_test, endIndices_test)\n",
    "print(\"--- Time to load the test files: %s seconds ---\" % (time.time() - start_time), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Time to load the network: 0.10053396224975586 seconds ---\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "net.to(device)  #GPU\n",
    "path_net = workPath+'fullNet_gpu_90Cosmo_30epoch.pth'\n",
    "net.load_state_dict(torch.load(path_net, map_location=torch.device('cpu')))  # delet the cpu option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Testing time: 108.16155290603638 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "\n",
    "start_time = time.time() \n",
    "\n",
    "n_batch_test = int(n_testimages/n_perbatch_test)\n",
    "targets_temp = np.concatenate((np.array([Omegas_test]).T, np.array([sigmas_test]).T), axis=1) # temp stacked targets of all comsos being trained\n",
    "targets_test = np.repeat(targets_temp, n_batch_test, axis=0)  # repeat each set of parameters by the number of batches\n",
    "#outputs_test = test_cnn(net, inputs_test, n_perbatch_test, False)  # not tracking loss\n",
    "criterion = nn.L1Loss()  # MAE loss \n",
    "outputs_test, losses_test = test_cnn(net, inputs_test, n_perbatch_test, \n",
    "                                                 True, loss_fn=criterion, target=targets_test)    \n",
    "    \n",
    "\n",
    "## print('Epoch %d test loss: %.3f' % (epoch + 1, np.sum(losses_test)/len(losses_test)), flush=True)\n",
    "## losseslist_test.append(np.sum(losses_validate)/len(losses_validate))\n",
    "\n",
    "print(\"--- Testing time: %s seconds ---\" % (time.time() - start_time), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Table([outputs_test[:,0], outputs_test[:,1], np.ndarray.flatten(losses_test)], names=('Om_pred', 'si_pred', 'Losses'))\n",
    "t.write(workPath+'fullNet_testing.dat', format='ascii', overwrite=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2402521  0.24192996 0.2640354  0.23442587 0.24141914 0.25778732\n",
      " 0.24404906 0.23587981 0.24339312 0.23608756 0.22818284 0.2669407\n",
      " 0.24347457 0.26057404 0.2695773  0.26412612 0.23725295 0.23034894\n",
      " 0.2562923  0.23679775 0.2314605  0.25010455 0.25244623 0.22545518\n",
      " 0.24118912 0.26298046 0.24508491 0.25230333 0.23755784 0.2330363\n",
      " 0.24608228 0.23817505 0.24577206 0.24046235 0.235742   0.24094792\n",
      " 0.25538236 0.24315317 0.2245759  0.25516966 0.25729108 0.23314254\n",
      " 0.25140643 0.23515302 0.23051041 0.2622915  0.2370539  0.23921925\n",
      " 0.24312192 0.23759134 0.2435503  0.22087522 0.2378199  0.2497895\n",
      " 0.23029692 0.24009512 0.24186599 0.24455848 0.24798293 0.24157794\n",
      " 0.25575858 0.23048978 0.26009265 0.25863948 0.25188035 0.23765394\n",
      " 0.2639043  0.2534419  0.2588077  0.25156787 0.24000975 0.24882916\n",
      " 0.25539374 0.2509758  0.2457375  0.26622602 0.24732047 0.26414075\n",
      " 0.26247352 0.27066088 0.25355196 0.24339665 0.2542204  0.2400224\n",
      " 0.23716442 0.259849   0.24480706 0.23798664 0.24849162 0.26846826\n",
      " 0.25416687 0.25487125 0.24891716 0.25347987 0.25221127 0.24472752\n",
      " 0.25201228 0.24072167 0.24498907 0.24514817 0.24339113 0.23839098\n",
      " 0.23878534 0.25042337 0.2632833  0.24525666 0.25054216 0.25567856\n",
      " 0.23414133 0.27063632 0.23654775 0.25223425 0.24985793 0.24319342\n",
      " 0.24974443 0.23746818 0.24686153 0.2502463  0.23508376 0.24043603\n",
      " 0.24262296 0.25354648 0.25439328 0.24998586 0.24519646 0.23857856\n",
      " 0.25574598 0.25946066]\n",
      "[0.89443076 0.8931199  0.9153534  0.8729508  0.8918295  0.87918454\n",
      " 0.87421954 0.8485238  0.8904122  0.89242303 0.8987972  0.90888\n",
      " 0.8877938  0.8710402  0.8649262  0.8590459  0.9088967  0.8768\n",
      " 0.8669675  0.89901036 0.88571477 0.9155713  0.8766694  0.891631\n",
      " 0.90143055 0.8767029  0.85819054 0.91416395 0.8893546  0.8885134\n",
      " 0.8769397  0.8815198  0.89567316 0.87953025 0.9053718  0.8998983\n",
      " 0.92856675 0.89685136 0.914031   0.8353849  0.9028482  0.8963889\n",
      " 0.88403016 0.8961926  0.8895749  0.89382994 0.91329515 0.88410676\n",
      " 0.8925309  0.8958752  0.9031431  0.90779436 0.89031994 0.89349955\n",
      " 0.88382053 0.89572835 0.9088738  0.88994443 0.9102342  0.9047121\n",
      " 0.8792052  0.90194196 0.87481844 0.8593217  0.94117236 0.9599759\n",
      " 0.9595976  0.9473904  0.9475131  0.9226248  0.923098   0.959005\n",
      " 0.9578934  0.951859   0.94516706 0.9363689  0.9518856  0.9082849\n",
      " 0.89826655 0.9131665  0.95833945 0.9301479  0.9232711  0.9642631\n",
      " 0.95219755 0.9255497  0.9561177  0.9579401  0.9605788  0.9021664\n",
      " 0.9265296  0.95137036 0.94905925 0.91137064 0.943988   0.94035184\n",
      " 0.8916522  0.9318578  0.93069386 0.9059655  0.93446743 0.9125096\n",
      " 0.91759    0.8905082  0.91658044 0.9191949  0.89556146 0.9005778\n",
      " 0.92825115 0.89461225 0.9264517  0.92600125 0.9295869  0.91810805\n",
      " 0.9218857  0.9299483  0.9306903  0.90834534 0.8908675  0.9508928\n",
      " 0.94427043 0.9032692  0.90285724 0.9265512  0.9460182  0.94827294\n",
      " 0.9044138  0.89032   ]\n"
     ]
    }
   ],
   "source": [
    "Om_predict = outputs_test[:,0]\n",
    "si_predict = outputs_test[:,1]\n",
    "print(Om_predict)\n",
    "print(si_predict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
