{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.io import fits\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "\n",
    "import time\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define constants etc.  \n",
    "\n",
    "rootpath = '/n/holyscratch01/dvorkin_lab/Users/tianliwang/maps_unzipped/'  # location of maps \n",
    "\n",
    "# these parameters are defined in the paper \n",
    "n_epoch = 5     # number of epochs to run \n",
    "n_imagespercosmo = 512  # number of images per cosmology \n",
    "n_perbatch = 32         # batch size for training \n",
    "n_perbatch_validate = n_perbatch  # set it to the same as training for now \n",
    "n_batch = n_imagespercosmo/n_perbatch  # number of minibatches \n",
    "dim_image = 1024 \n",
    "dim_downsized = int(dim_image/2)            # downsized image dimension \n",
    "\n",
    "trainFraction = 10.0/16  # fraction of images per cosmology for training \n",
    "validateFraction = 3.0/16\n",
    "testFraction = 1-trainFraction-validateFraction\n",
    "\n",
    "n_cosmosToTrain = 2 \n",
    "n_trainimages = int(n_imagespercosmo*trainFraction)    # number of images per cosmology to train\n",
    "n_validateimages = int(n_imagespercosmo*validateFraction)   # number of images per cosmology to validate\n",
    "\n",
    "# start and end image indices for training and validating \n",
    "startIndices_train = np.ones(n_cosmosToTrain).astype(int)\n",
    "endIndices_train = n_trainimages*np.ones(n_cosmosToTrain).astype(int)\n",
    "startIndices_validate = (n_trainimages+1)*np.ones(n_cosmosToTrain).astype(int)\n",
    "endIndices_validate = (n_trainimages+n_validateimages)*np.ones(n_cosmosToTrain).astype(int)\n",
    "\n",
    "learning_rate = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_parametersfromfile(path):\n",
    "    '''\n",
    "    Input: string for the directory to the folders of cosmology \n",
    "    \n",
    "    Function: reads the Omega_m and sigma_8 values from the folder names in \n",
    "    the path directory and store them as arrays of strings. Stores the values \n",
    "    as strings to avoid rounding, for example, 0.600 to 0.6. \n",
    "    \n",
    "    Output: (array of strings of Omega_m, array of strings of sigma_8)\n",
    "    '''\n",
    "    \n",
    "    Om_strings = [] \n",
    "    si_strings = []\n",
    "    \n",
    "    for filename in os.listdir(path):\n",
    "        if (filename.startswith('Om') and not filename.endswith('.gz')): \n",
    "            Om_strings.append(filename[2:7]) \n",
    "            si_strings.append(filename[10:15])\n",
    "            \n",
    "    return Om_strings, si_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(path, Oms, sis, start_indices, end_indices): \n",
    "    '''\n",
    "    Input: file root directory path, strings of omegam and sigma8 values, \n",
    "    start image indices and end image indices. Oms, sis, start_indices, end_indices\n",
    "    should be arrays of same length. \n",
    "    \n",
    "    Function: reads the images labeled between start_indices[i] and end_indices[i] (inclusive)\n",
    "    for each of the ith cosmologies specified by omegam and sigma8 values \n",
    "    \n",
    "    Output: numpy array of 3D images\n",
    "    '''\n",
    "    \n",
    "    n_cosmos = len(Oms)  # this should be shared by Oms, sis, start_indices, end_indices \n",
    "    \n",
    "    # read in the files and put them into 3D matrix \n",
    "    filedata = []\n",
    "\n",
    "    for j in range(0, n_cosmos): \n",
    "        i_start = start_indices[j]\n",
    "        i_end = end_indices[j]\n",
    "        Om = Oms[j]\n",
    "        si = sis[j]\n",
    "        \n",
    "        for i in range(i_start, i_end+1): \n",
    "            if (i < 10): filenum = '00{}'.format(i)\n",
    "            elif (i < 100): filenum = '0{}'.format(i)\n",
    "            else: filenum = i \n",
    "\n",
    "            hdulist = fits.open('{}Om{}_si{}/WLconv_z1.00_0{}r.fits'.format(path, Om, si, filenum))\n",
    "            image = hdulist[0].data\n",
    "\n",
    "            # downsize from dim_image to dim_downsized \n",
    "            image_downsized = image.reshape([dim_downsized, dim_image//dim_downsized, \n",
    "                                    dim_downsized, dim_image//dim_downsized]).mean(3).mean(1)\n",
    "\n",
    "            # newaxis makes the image 3D (1x512x512) \n",
    "            filedata.append(image_downsized[np.newaxis,:,:])\n",
    "    \n",
    "    return np.array(filedata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_network(network):\n",
    "    '''\n",
    "    Input: CNN object \n",
    "    \n",
    "    Function: resetting the parameters in all the layers of network \n",
    "    '''\n",
    "    for layer in network.children():     \n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "            layer.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_cnn(network, testinputs, batchsize, isTrackingloss, **kwargs): \n",
    "    '''\n",
    "    Input: CNN object, testing data, batch size for testing, boolean for whether \n",
    "    to track losses bewteen test ouputs and targets, **kwargs includes the loss function\n",
    "    criterion and the targetting outputs. Target should be in the form of \n",
    "    an array of (omegam, sigma8) parameters each corresponding to the target of one batch. \n",
    "    \n",
    "    Call with \n",
    "    test_cnn(network, testinputs, batchsize, False) \n",
    "    or \n",
    "    test_cnn(network, testinputs, batchsize, True, loss_fn=loss_fn, target=target)\n",
    "    \n",
    "    Function: Run the network with test inputs and specified batch size and track loss as \n",
    "    requested. \n",
    "    \n",
    "    Output: \n",
    "    If isTrackingloss is True, return (np array of predicted outputs, np array of losses for each batch)\n",
    "    If isTrackingloss is False, return np array of predicted outputs\n",
    "    '''\n",
    "    \n",
    "    # unpack optional arguments **kwargs \n",
    "    if (isTrackingloss): \n",
    "        loss_fn = kwargs.get('loss_fn', None)\n",
    "        testtargets = kwargs.get('target', None)\n",
    "        losses = []\n",
    "    \n",
    "    # make the data into 4D tensor and put each batch into iterable \n",
    "    testloader = torch.utils.data.DataLoader(testinputs, batch_size=batchsize, shuffle=False)\n",
    "    \n",
    "    predictions = []  # stores outputs of network with test data inputs \n",
    "    \n",
    "    with torch.no_grad(): \n",
    "        for i, testdata in enumerate(testloader, 0):\n",
    "            testoutputs = network(testdata)\n",
    "            predictions.append(testoutputs[0].numpy()) \n",
    "\n",
    "            if (isTrackingloss): \n",
    "                testtarget = torch.tensor(testtargets[i]).repeat(batchsize, 1)  \n",
    "                losses.append(loss_fn(testoutputs, testtarget))\n",
    "    \n",
    "    if (isTrackingloss): \n",
    "        return np.array(predictions), np.array(losses)\n",
    "    \n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network structure \n",
    "\n",
    "class Net (nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.pool2 = nn.AvgPool2d(2, 2)\n",
    "        self.pool4 = nn.AvgPool2d(4, 4)\n",
    "        self.pool6 = nn.AvgPool2d(6, 6)\n",
    "        \n",
    "        self.conv1to32_5 = nn.Conv2d(1, 32, 5)\n",
    "        self.conv32to64_5 = nn.Conv2d(32, 64, 5)\n",
    "        self.conv64to128_5 = nn.Conv2d(64, 128, 5)\n",
    "        self.conv128to256_5 = nn.Conv2d(128, 256, 5)\n",
    "        self.conv256to512_5 = nn.Conv2d(256, 512, 5)\n",
    "        self.conv512to256_5 = nn.Conv2d(512, 216, 5)\n",
    "        self.conv256to512_3 = nn.Conv2d(216, 512, 3)\n",
    "        \n",
    "        self.fc = nn.Linear(512, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool2(F.relu(self.conv1to32_5(x)))  # out: 254x254x32 \n",
    "        x = self.pool2(F.relu(self.conv32to64_5(x)))  # out: 125x125x64 \n",
    "        x = self.pool2(F.relu(self.conv64to128_5(x)))  # out: 60x60x128 \n",
    "        x = self.pool2(F.relu(self.conv128to256_5(x)))  # out: 28x28x256 \n",
    "        x = self.pool2(F.relu(self.conv256to512_5(x)))  # out: 12x12x512\n",
    "        x = F.relu(self.conv512to256_5(x))  # out: 8x8x256 \n",
    "        x = self.pool6(F.relu(self.conv256to512_3(x)))  # out: 6x6x512->1x1x512\n",
    "        \n",
    "        x = x.view(-1, 512)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function and training rate defined by paper \n",
    "criterion = nn.L1Loss()  # MAE loss \n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in all the omegam and sigma8 values from the directory \n",
    "Omegam_strings, sigma8_strings = read_parametersfromfile(rootpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "Omegastrings_train = Omegam_strings[:n_cosmosToTrain]\n",
    "sigmastrings_train = sigma8_strings[:n_cosmosToTrain]\n",
    "Omegas_train = np.array(Omegastrings_train).astype(np.float)\n",
    "sigmas_train = np.array(sigmastrings_train).astype(np.float)\n",
    "\n",
    "# use the same sets of params for validation as training for now (can change as appropriate)\n",
    "Omegastrings_validate = Omegastrings_train\n",
    "sigmastrings_validate = sigmastrings_train\n",
    "Omegas_validate = np.array(Omegastrings_validate).astype(np.float)\n",
    "sigmas_validate = np.array(sigmastrings_validate).astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Time to load the files: 61.754932165145874 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()  \n",
    "\n",
    "inputs_train = read_files(rootpath, Omegastrings_train, sigmastrings_train, startIndices_train, endIndices_train)\n",
    "inputs_validate = read_files(rootpath, Omegastrings_validate, sigmastrings_validate, startIndices_validate, endIndices_validate)\n",
    "\n",
    "print(\"--- Time to load the files: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paramPairs_train = torch.cat((torch.tensor(Omegas_train).view(-1,1), torch.tensor(sigmas_train).view(-1,1)), axis=1)\n",
    "#target_train = paramPairs_train.unsqueeze(1).repeat(1,n_perbatch,1).view(-1,2)\n",
    "\n",
    "# make the data into 4D tensor and put each batch into iterable \n",
    "trainloader = torch.utils.data.DataLoader(inputs_train, batch_size=n_perbatch, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()  # to time the training\n",
    "\n",
    "for epoch in range(n_epoch): \n",
    "    \n",
    "    running_loss = 0.0   # running loss of training per epoch\n",
    "    \n",
    "    for i, data in enumerate(trainloader, 0): \n",
    "        # update target value of sigma8 and omegam (stacked to the correct dimension to match the number per batch)\n",
    "        if (i % n_trainimages == 0): \n",
    "            params_index = i//n_trainimages  \n",
    "            \n",
    "            # change target every n_trainimages images \n",
    "            target = torch.tensor([Omegas_train[params_index], sigmas_train[params_index]]).repeat(n_perbatch, 1)  \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        outputs = net(data)\n",
    "        loss = criterion(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print('Epoch %d training loss: %.3f' % (epoch + 1, running_loss/len(trainloader)))\n",
    "    \n",
    "    # Validation \n",
    "    n_batch_validate = int(n_validateimages/n_perbatch_validate)\n",
    "    targets_temp = np.concatenate((np.array([Omegas_validate]).T, np.array([sigmas_validate]).T), axis=1) # temp stacked targets of all comsos being trained\n",
    "    targets_validate = np.repeat(targets_temp, n_batch_validate, axis=0)  # repeat each set of parameters by the number of batches\n",
    "    outputs_validate, losses_validate = test_cnn(net, inputs_validate, n_perbatch_validate, \n",
    "                                                 True, loss_fn=criterion, target=targets_validate)\n",
    "    \n",
    "    print('Epoch %d validation loss: %.3f' % (epoch + 1, np.sum(losses_validate)/n_batch_validate))\n",
    "        \n",
    "print(\"--- Training time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
